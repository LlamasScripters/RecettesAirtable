# RecettesAirtable ğŸ³

Application web complÃ¨te de gestion de recettes avec gÃ©nÃ©ration automatique par IA, utilisant Airtable comme base de donnÃ©es et Ollama pour l'intelligence artificielle.

## âœ¨ FonctionnalitÃ©s

- **GÃ©nÃ©ration de recettes par IA** : CrÃ©ez des recettes personnalisÃ©es Ã  partir d'ingrÃ©dients
- **Gestion complÃ¨te** : Ajout, modification, suppression de recettes
- **Filtrage avancÃ©** : Par type, difficultÃ©, allergÃ¨nes
- **Analyse nutritionnelle** : Calcul automatique des valeurs nutritionnelles
- **Interface moderne** : Frontend React avec Tailwind CSS
- **API REST** : Backend Node.js/Express sÃ©curisÃ©

## ğŸ—ï¸ Architecture

```
RecettesAirtable/
â”œâ”€â”€ client/          # Frontend React/Vite
â”œâ”€â”€ server/          # Backend Node.js/Express
â”œâ”€â”€ compose.yml      # Configuration Docker
â””â”€â”€ README.md
```

### Stack technologique

**Frontend :**
- React 19 + TypeScript
- Vite (build tool)
- TanStack Router (routing)
- TanStack Query (state management)
- Tailwind CSS + Radix UI (styling)
- Zustand (store)

**Backend :**
- Node.js + Express + TypeScript
- Airtable (base de donnÃ©es)
- Ollama (IA locale)
- Zod (validation)
- Helmet + CORS (sÃ©curitÃ©)

**DevOps :**
- Docker + Docker Compose
- Hot-reload en dÃ©veloppement
- Nginx (proxy frontend)

## ğŸš€ DÃ©marrage rapide

### PrÃ©requis

- Docker et Docker Compose
- AccÃ¨s Ã  une base Airtable

### Installation

1. **Cloner le projet :**
```bash
git clone <repo-url>
cd RecettesAirtable
```

2. **Configurer les variables d'environnement :**
CrÃ©er `server/.env` avec vos clÃ©s Airtable :
```env
AIRTABLE_API_KEY=your_api_key
AIRTABLE_BASE_ID=your_base_id
```

3. **DÃ©marrer l'application :**
```bash
docker compose up -d
```

4. **TÃ©lÃ©charger le modÃ¨le IA :**
```bash
docker exec -it recettes-ollama ollama pull llama3.2:1b
```

5. **AccÃ©der Ã  l'application :**
- Frontend : http://localhost:3000
- API : http://localhost:5000
- Ollama : http://localhost:11434

## ğŸ“Š Services Docker

| Service | Port | Description |
|---------|------|-------------|
| `recettes-client` | 3000 | Frontend React |
| `recettes-server` | 5000 | API Node.js |
| `recettes-ollama` | 11434 | Service IA Ollama |

## ğŸ”§ Commandes utiles

```bash
# Voir les logs
docker compose logs -f

# RedÃ©marrer un service
docker compose restart server

# ArrÃªter l'application
docker compose down

# Reconstruire les images
docker compose build

# ModÃ¨les IA disponibles
docker exec recettes-ollama ollama list
```

## ğŸ¯ Utilisation

1. **AccÃ©der au frontend** sur http://localhost:3000
2. **Consulter les recettes** existantes dans Airtable
3. **GÃ©nÃ©rer une nouvelle recette** avec l'IA :
   - SÃ©lectionner des ingrÃ©dients
   - Choisir le nombre de portions
   - SpÃ©cifier allergÃ¨nes et prÃ©fÃ©rences
   - Laisser l'IA crÃ©er la recette
4. **Filtrer et rechercher** dans la collection

## ğŸ“š Documentation

- [Documentation API Backend](./server/README.md)
- [Guide Frontend](./client/README.md)

## ğŸ” Configuration Airtable

Structure de base requise :

**Table "recipes" :**
- `titre` (Text)
- `description` (Long text)
- `type` (Single select)
- `difficultÃ©` (Single select) 
- `ingredients` (Long text)
- `instructions` (Long text)
- `portions` (Number)
- `tempsPrÃ©paration` (Number)
- `tempsCuisson` (Number)
- `allergenes` (Multiple select)
- `dateCreation` (Date)

## ğŸ¤– Configuration IA

L'application utilise **Ollama** pour la gÃ©nÃ©ration de recettes. ModÃ¨les recommandÃ©s :

- `tinyllama` (637MB) - Rapide, basique
- `llama3.2:1b` (1.3GB) - Ã‰quilibrÃ©
- `llama3.2` (2GB) - Meilleure qualitÃ©

```bash
# Changer de modÃ¨le
docker exec -it recettes-ollama ollama pull <model-name>
# Puis modifier OLLAMA_MODEL dans compose.yml
```

## ğŸ› ï¸ DÃ©veloppement

### Mode dÃ©veloppement

```bash
# DÃ©marrer avec hot-reload
docker compose up -d

# Ou manuellement :
cd server && npm run dev
cd client && npm start
```

### Tests

```bash
# Backend
cd server && npm test

# Frontend  
cd client && npm test
```

## ğŸ› DÃ©pannage

**Erreur "modÃ¨le non trouvÃ©" :**
```bash
docker exec -it recettes-ollama ollama pull llama3.2:1b
```

**Frontend inaccessible :**
VÃ©rifier que Vite Ã©coute sur `0.0.0.0:3000`

**API erreur 500 :**
VÃ©rifier les variables d'environnement Airtable

## ğŸ“„ Licence

MIT License

## ğŸ‘¥ Contributeurs

- DÃ©veloppÃ© dans le cadre du M2 Airtable AvancÃ©